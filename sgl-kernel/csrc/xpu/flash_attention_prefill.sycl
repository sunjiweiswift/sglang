#include "cutlass/epilogue/collective/default_epilogue.hpp"
#include "cutlass/gemm/device/gemm_universal_adapter.h"
#include "flash_attention_v2/collective/fmha_fusion.hpp"
#include "flash_attention_v2/kernel/tile_scheduler.hpp"
#include "cutlass/util/packed_stride.hpp"
#include "flash_attention_v2/kernel/xe_flash_attn_prefill.hpp"
#include "flash_attention_v2/collective/xe_flash_attn_prefill_epilogue.hpp"
#include "flash_attention_v2/collective/xe_flash_attn_prefill_softmax_epilogue.hpp"
#include "cutlass/util/GPU_Clock.hpp"
#include "cutlass/util/sycl_event_manager.hpp"

// #include <ATen/ATen.h>
// #include <ATen/Parallel.h>
// #include <c10/xpu/XPUStream.h>
// #include <torch/all.h>
// #include <torch/python.h>

// #include <cmath>
// #include <cstdint>
// #include <iostream>
// #include <sycl/sycl.hpp>
// #include <vector>

// #include "SYCLHelpers.h"

// std::vector<at::Tensor> flash_attention_v2(
//     at::Tensor& q,        // (b, s_q, h, d) or (total_q, h, d) if there is cu_seqlens_q
//     const at::Tensor& k,  // (b_k, s_k, h_k, d) or (total_k, h_k, d) if there is cu_seqlens_k or (num_pages,
//     page_size,
//                           // h_k, d) if there is page_table.
//     const at::Tensor& v,  // (b_k, s_k, h_k, dv) or (total_k, h_k, dv) if there is cu_seqlens_k or (num_pages,
//                           // page_size, h_k, dv) if there is page_table.
//     std::optional<const at::Tensor>&
//         k_new_,  // (b, s_k_new, h_k, d) or (total_k_new, h_k, d) if there is cu_seqlens_k_new
//     std::optional<const at::Tensor>&
//         v_new_,  // (b, s_k_new, h_k, dv) or (total_k_new, h_k, dv) if there is cu_seqlens_k_new
//     std::optional<const at::Tensor>& q_v_,           // (b, s_q, h, dv) or (total_q_new, h, dv) if there is
//     cu_seqlens_q std::optional<at::Tensor>& out_,                 // (b, s_q, h, dv) or (total_q, h, dv) if there is
//     cu_seqlens_q std::optional<const at::Tensor>& cu_seqlens_q_,  // b+1 std::optional<const at::Tensor>&
//     cu_seqlens_k_,  // b+1 std::optional<const at::Tensor>& cu_seqlens_k_new_,  // b+1 std::optional<const
//     at::Tensor>&
//         seqused_q_,  // b. If given, only this many elements of each batch element's queries and outputs are used.
//     std::optional<const at::Tensor>&
//         seqused_k_,  // b. If given, only this many elements of each batch element's keys are used.
//     std::optional<int> max_seqlen_q_,
//     // TODO: check if we need max_seqlen_k
//     std::optional<int> max_seqlen_k_,
//     std::optional<const at::Tensor>& page_table_,      // (b_k, max_num_pages_per_seq)
//     std::optional<const at::Tensor>& kv_batch_idx_,    // b. indices to index into the KV cache
//     std::optional<const at::Tensor>& leftpad_k_,       // b
//     std::optional<const at::Tensor>& rotary_cos_,      // seqlen_ro x (rotary_dim / 2)
//     std::optional<const at::Tensor>& rotary_sin_,      // seqlen_ro x (rotary_dim / 2)
//     std::optional<const at::Tensor>& seqlens_rotary_,  // b
//     std::optional<at::Tensor>& q_descale_,             // (b, h_k), not (b, h)
//     std::optional<at::Tensor>& k_descale_,             // (b, h_k)
//     std::optional<at::Tensor>& v_descale_,             // (b, h_k)
//     float const softmax_scale,
//     bool is_causal,
//     int window_size_left,
//     int window_size_right,
//     float const softcap,
//     bool const is_rotary_interleaved,  // if true, rotary combines indices 0 & 1, else indices 0 & rotary_dim / 2
//     std::optional<at::Tensor>& scheduler_metadata_,  // (b + 1)
//     int num_splits,
//     std::optional<bool> pack_gqa_,
//     int const sm_margin) {}
template <typename DispatchPolicy, typename input, typename output>
struct MMAOP {
  static_assert(
      cutlass::detail::dependent_false<DispatchPolicy>,
      "Could not find a supported MMA ATOM Operation for flash attention");
};

template <typename DispatchPolicy>
struct MMAOP<DispatchPolicy, bfloat16_t, float> {
  using Type = cute::XE_8x16x16_F32BF16BF16F32_TT;
};

template <typename DispatchPolicy>
struct MMAOP<DispatchPolicy, half_t, float> {
  using Type = cute::XE_8x16x16_F32F16F16F32_TT;
};

template <
    typename ElementInputType,
    typename ElementAccumulatorType,
    typename ElementOutputType,
    typename GmemTiledCopyQ,
    typename GmemTiledCopyK,
    typename GmemTiledCopyV,
    typename GmemTiledCopyO,
    typename TileShapeQK,
    typename TileShapePV,
    typename TileShapeOutput,
    typename SubgroupLayout,
    bool HasCausal,
    bool IsVarLen,
    int PipelineStages>
struct FMHAPrefillConfig {
  using ElementOutput = ElementOutputType;            // <- data type of output
  using ElementInputQ = ElementInputType;             // <- data type of elements in input matrix Q
  using ElementInputK = ElementInputType;             // <- data type of elements in input matrix K
  using ElementInputV = ElementInputType;             // <- data type of elements in input matrix V
  using ElementAccumulator = ElementAccumulatorType;  // <- data type of accumulator for mma operation
  using LayoutQ = cutlass::layout::RowMajor;
  using LayoutK = cutlass::layout::ColumnMajor;
  using LayoutV = cutlass::layout::RowMajor;
  using LayoutO = cutlass::layout::RowMajor;
  static constexpr bool Causal = HasCausal;
  static constexpr bool VarLen = IsVarLen;
  using GEMMDispatchPolicy = cutlass::gemm::MainloopIntelXeXMX16<PipelineStages>;
  using EpilogueDispatchPolicy = cutlass::epilogue::IntelXeXMX16;
  //   using MMAOperation = XE_8x16x16_F32BF16BF16F32_TT;
  using MMAOperation = typename MMAOP<GEMMDispatchPolicy, ElementInputType, ElementAccumulator>::Type;
  using CollectiveEpilogue = cutlass::flash_attention::collective::FlashPrefillEpilogue<
      EpilogueDispatchPolicy,
      MMAOperation,
      TileShapeOutput,
      SubgroupLayout,
      ElementAccumulator,
      cutlass::gemm::TagToStrideC_t<LayoutO>,
      ElementOutput,
      GmemTiledCopyO>;

  using CollectiveSoftmaxEpilogue = cutlass::flash_attention::collective::
      FlashPrefillSoftmaxEpilogue<Causal, EpilogueDispatchPolicy, ElementAccumulator>;

  using ProblemShapeRegular = cute::tuple<int, int, int, int, int, int, int>;
  using ProblemShapeVarlen =
      cute::tuple<int, int, int, cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, int, int>;
  using ProblemShapeType = std::conditional_t<VarLen, ProblemShapeVarlen, ProblemShapeRegular>;

  // Mainloop
  using CollectiveMainloop = cutlass::flash_attention::collective::FlashPrefillMma<
      GEMMDispatchPolicy,
      ProblemShapeType,
      ElementInputQ,
      cutlass::gemm::TagToStrideA_t<LayoutQ>,
      ElementInputK,
      cutlass::gemm::TagToStrideB_t<LayoutK>,
      ElementInputV,
      cutlass::gemm::TagToStrideB_t<LayoutV>,
      MMAOperation,
      TileShapeQK,
      TileShapePV,
      SubgroupLayout,
      GmemTiledCopyQ,
      GmemTiledCopyK,
      GmemTiledCopyV,
      Causal>;

  using GemmKernel = cutlass::flash_attention::kernel::
      FMHAPrefill<ProblemShapeType, CollectiveMainloop, CollectiveSoftmaxEpilogue, CollectiveEpilogue>;
};