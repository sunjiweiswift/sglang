# If the device is Battlemage, we need to set UBUNTU_VERSION to 24.10

# Usage: docker build --build-arg UBUNTU_VERSION=24.10 --build-arg PYTHON_VERSION=3.10 --build-arg GITHUB_TOKEN=github_token -t sglang:xpu_u2410 -f  Dockerfile.xpu --no-cache .


# Set default Ubuntu version to 24.04
ARG UBUNTU_VERSION = 24.04
FROM ubuntu: $UBUNTU_VERSION

# Set environment variables
ARG UBUNTU_VERSION = 24.04
ENV UBUNTU_VERSION =${UBUNTU_VERSION}
ENV DEBIAN_FRONTEND = noninteractive

# Set maintainer label
LABEL maintainer = "zhaoqiong.zheng@intel.com"

# Define build arguments
ARG PYTHON_VERSION = 3.10
ARG GITHUB_TOKEN
ENV GITHUB_TOKEN =${GITHUB_TOKEN}

# Install general packages
RUN apt-get update & & \
    apt-get install - y - -no-install-recommends \
    apt-utils \
    build-essential \
    openssl \
    ca-certificates \
    curl \
    jq \
    git \
    gnupg2 \
    gpg-agent \
    rsync \
    sudo \
    unzip \
    vim \
    wget & & \
    apt-get clean & & \
    rm - rf / var/lib/apt/lists/*

# Downgrade gcc version on ubuntu 24.10 OS for building PyTorch from source
RUN if ["$UBUNTU_VERSION"= "24.10"]
thenecho "UBUNTU_VERSION is 24.10, proceeding alternate gcc/g++ to gcc-13/g++-13 ..." & &apt-get update & &apt-get install - y gcc-13 g++-13 & &update-alternatives - -install / usr/bin/gcc gcc / usr/bin/gcc-13 200 & &update-alternatives - -set gcc / usr/bin/gcc-13 & &update-alternatives - -install / usr/bin/g++ g++ / usr/bin/g++-13 200 & &update-alternatives - -set g++ / usr/bin/g++-13 & &apt-get clean & &rm - rf / var/lib/apt/lists/*
fi

# Install Intel Driver based on Ubuntu version
RUN if ["$UBUNTU_VERSION" = "24.10"]
thenecho "UBUNTU_VERSION is 24.10, proceeding with driver installation..." & &apt-get update & &apt-get install - y software-properties-common & &add-apt-repository - y ppa: kobuk-team/intel-graphics & &apt-get install - y libze-intel-gpu1 = 24.52.32224.14-1~24.10~ppa2 libze1 = 1.19.2.0-1076~24.10 intel-metrics-discovery = 1.13.178-0ubuntu1~24.10~ppa1 intel-opencl-icd = 24.52.32224.14-1~24.10~ppa2 clinfo intel-gsc = 0.9.5-0ubuntu1~24.10~ppa1 & &apt-get install - y intel-media-va-driver-non-free = 25.1.0-0ubuntu1~ppa1 libmfx1 = 22.5.4-1ubuntu1 libmfx-gen1 = 25.1.0-0ubuntu1~24.10~ppa1 libvpl2 = 1: 2.14.0-0ubuntu1~24.10~ppa1 libvpl-tools = 1.3.0-0ubuntu1~24.10~ppa1 libva-glx2 = 2.22.0-1 va-driver-all = 2.22.0-1 vainfo = 2.22.0+ds1-1 & &apt-get install - y libze-dev = 1.19.2.0-1076~24.10 intel-ocloc = 24.52.32224.14-1~24.10~ppa2 & &dpkg - l | grep libze & &apt-get clean & &rm - rf / var/lib/apt/lists/*
elif ["$UBUNTU_VERSION" = "24.04"]; thenecho "UBUNTU_VERSION is 24.04, proceeding with driver installation..." & &apt-get update & &wget - qO - https:
    // repositories.intel.com/gpu/intel-graphics.key | sudo gpg - -yes - -dearmor - -output / usr/share/keyrings/intel-graphics.gpg & &echo "deb [arch=amd64,i386 signed-by=/usr/share/keyrings/intel-graphics.gpg] https://repositories.intel.com/gpu/ubuntu noble unified" | sudo tee / etc/apt/sources.list.d/intel-gpu-noble.list & &apt-get update & &apt-get install - y libze-intel-gpu1 = 25.05.32567.18-1099~24.04 libze1 = 1.20.2.0-1098~24.04 intel-opencl-icd = 25.05.32567.18-1099~24.04 clinfo intel-gsc = 0.9.5-113~u24.04 & &apt-get install - y libze-dev = 1.20.2.0-1098~24.04 intel-ocloc = 25.05.32567.18-1099~24.04 & &apt-get clean & &rm - rf / var/lib/apt/lists/*
    fi

# Install Deep Learning Essetial for requirede oneapi packages
RUN echo "Installing Deep Learning Essetial for building PyTorch from source..." \
    apt-get update & & \
    sudo apt install - y gpg-agent wget gnupg & & \
    wget - qO - https: // apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | \
    sudo gpg - -dearmor - o / usr/share/keyrings/oneapi-archive-keyring.gpg & & \
    echo "deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main" | \
    sudo tee / etc/apt/sources.list.d/oneAPI.list & & \
    apt-get update & & \
    apt-get install - y intel-oneapi-dpcpp-cpp-2025.0 = 2025.0.4-1519 \
    intel-oneapi-mkl-devel = 2025.0.1-14 \
    intel-oneapi-ccl-devel = 2021.14.0-505 \
    intel-pti-dev = 0.10.0-284 \
    intel-oneapi-dnnl = 2025.0.1-6 \
    intel-oneapi-umf-0.9 & & \
    apt-get clean & & \
    rm - rf / var/lib/apt/lists/*

# Install Miniforge & PyTorch/Triton & build vllm/SGlang from source
RUN curl - fsSL - v - o miniforge.sh - O https: // github.com/conda-forge/miniforge/releases/download/25.1.1-0/Miniforge3-Linux-x86_64.sh & & \
    bash miniforge.sh - b - p ./miniforge3 & & \
    rm miniforge.sh & & \
    . ./miniforge3/bin/activate & & \
    echo ${PYTHON_VERSION} | sed 's/\.//g' > /tmp/version.txt & & \
    export PYTHON_VERSION_STRING =$(cat / tmp/version.txt) & & \
    conda create - y - n py${PYTHON_VERSION_STRING} python =${PYTHON_VERSION} & & conda activate py${PYTHON_VERSION_STRING} & & \
    conda install pip & & \
    cd / root & & \
    TEMP_DIR =$(mktemp - d) & & \
    echo "Downloading Triton wheels..." & & \
    WORKFLOW_ID =$(curl - s - H "Authorization: token $GITHUB_TOKEN"
                   "https://api.github.com/repos/intel/intel-xpu-backend-for-triton/actions/workflows"
                   | jq - r '.workflows[] | select(.name == "Triton wheels") | .id') & & \
    echo "WORKFLOW_ID=${WORKFLOW_ID}" & & \
    RUN_ID =$(curl - s - H "Authorization: token $GITHUB_TOKEN"
              "https://api.github.com/repos/intel/intel-xpu-backend-for-triton/actions/workflows/${WORKFLOW_ID}/runs?status=completed&conclusion=success&per_page=1"
              | jq - r '.workflow_runs[0].id') & & \
    echo "RUN_ID=${RUN_ID}" & & \
    ARTIFACT_TEMPLATE = "wheels-pytorch-py${PYTHON_VERSION}*" & & \
    REGEX_PATTERN =$(echo "$ARTIFACT_TEMPLATE" | sed 's/\*/.*/g') & & \
    ARTIFACT_NAME =$(curl - s - H "Authorization: token $GITHUB_TOKEN"
                     "https://api.github.com/repos/intel/intel-xpu-backend-for-triton/actions/runs/$RUN_ID/artifacts" |
                     jq - r - -arg regex "^${REGEX_PATTERN}$"
                     '(.artifacts // [])[] | select(.name | test($regex)) | .name') & & \
    echo "ARTIFACT_NAME=${ARTIFACT_NAME}" & & \
    LATEST_ARTIFACT_URL =$(curl - s - H "Authorization: token $GITHUB_TOKEN"
                           "https://api.github.com/repos/intel/intel-xpu-backend-for-triton/actions/runs/$RUN_ID/artifacts" |
                           jq - r ".artifacts[] | select(.name == \"$ARTIFACT_NAME\") | .archive_download_url") & & \
    echo "LATEST_ARTIFACT_URL=${LATEST_ARTIFACT_URL}" & & \
    curl - L - H "Authorization: token $GITHUB_TOKEN" - C - \
    --retry 5 \
    - o $ARTIFACT_NAME.zip $LATEST_ARTIFACT_URL & & \
    unzip - t $ARTIFACT_NAME.zip & & \
    unzip $ARTIFACT_NAME.zip - d $TEMP_DIR & & \
    echo  "Installing Triton wheels..." & & \
    pip install $TEMP_DIR/*.whl - -root-user-action = ignore & & \
    rm - rf $TEMP_DIR & & \
    # Install vllm for Intel GPU from personal repo, will upstream later
cd / root & & \
    . / opt/intel/oneapi/setvars.sh & & \
    echo "Building vllm/sglang from source ..." & & \
    git clone https: // github.com/zhuyuhua-v/vllm.git & & \
    cd vllm & & \
    git checkout yuhua/deepseek & & \
    pip install setuptools_scm - -root-user-action = ignore & & \
    pip install setuptools == 75.6.0 packaging == 24.2 - -root-user-action = ignore & & \
    VLLM_TARGET_DEVICE = xpu python setup.py install & & \
    # Install SGlang from source
cd / root & & \
    git clone https: // github.com/sgl-project/sglang.git & & \
    cd sglang & & \
    pip install - -upgrade pip - -root-user-action = ignore & & \
    pip install - e "python[all_xpu]" - -root-user-action = ignore & & \
    # Install required packages for sglang workloads
pip install msgspec blake3 py-cpuinfo compressed_tensors gguf partial_json_parser einops - -root-user-action = ignore & & \
    conda install libsqlite = 3.48.0 - y & & \
    echo ". /miniforge3/bin/activate; conda activate py${PYTHON_VERSION_STRING}; . /opt/intel/oneapi/setvars.sh; cd /root/" >> /root/.bashrc


# Set the default shell to bash
SHELL["bash", "-c"]
CMD["bash", "-c", "source /root/.bashrc && exec bash"]
